{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF reader\n",
    "from PyPDF2 import PdfReader\n",
    "# langchain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "# Azure authentication\n",
    "from azure.identity import ClientSecretCredential\n",
    "import configs.constants as const\n",
    "import configs.deployments as dep\n",
    "# FAISS\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate to Azure\n",
    "credentials = ClientSecretCredential(const.IQVIA_TENANT_ID, const.SERVICE_PRINCIPAL, const.SERVICE_PRINCIPAL_SECRET)\n",
    "token = credentials.get_token(const.SCOPE_NON_INTERACTIVE)\n",
    "\n",
    "# gpt-35-turbo\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=f\"{const.OPENAI_API_BASE}/{const.OPENAI_API_TYPE}/{const.OPENAI_ACCOUNT_NAME}\",\n",
    "    openai_api_version=const.OPENAI_API_VERSION,\n",
    "    openai_api_key=token.token,\n",
    "    deployment_name=dep.GPT_35_TURBO,\n",
    "    openai_api_type=\"azure_ad\")\n",
    "\n",
    "# deployment-text-embedding-ada-002\t\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_base=f\"{const.OPENAI_API_BASE}/{const.OPENAI_API_TYPE}/{const.OPENAI_ACCOUNT_NAME}\",\n",
    "    openai_api_version=const.OPENAI_API_VERSION,\n",
    "    openai_api_key=token.token,\n",
    "    openai_api_type=\"azure_ad\",\n",
    "    deployment=dep.TEXT_EMBEDDING_ADA_002,\n",
    "    chunk_size=1)\n",
    "\n",
    "# Using the split_text method of the text_splitter object to split the raw_text string into chunks of 500 characters separated by newline character\n",
    "# text_splitter = CharacterTextSplitter(separator='\\n', chunk_size=500, chunk_overlap=0)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chain = load_qa_chain(llm, chain_type='stuff', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"C:\\\\Users\\\\u1128714\\\\Desktop\\\\prompt engineering\\\\MEASURING AND NARROWING.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "# Initializing an empty string variable called raw_text\n",
    "raw_text = ''\n",
    "# Using a for loop to iterate through the pages in the reader object and extract text from each page\n",
    "for i, page in enumerate(reader.pages):\n",
    "    # Extracting text from the current page using the extract_text() method\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "deduce previously unseen knowledge. For example, a model should be able to answer “How long\n",
      "was Queen Elizabeth’s reign?” even if the answer did not explicitly appear in the training data, by\n",
      "recalling her coronation and death dates and reasoning over these facts. While language models\n",
      "(LMs) have shown strong question answering performance, it remains unclear how much is due to\n",
      "memorization of huge corpora vs how much is due to reasoning.\n",
      "First, we quantify the reasoning abilities of LMs using multi-hop question answering. We present\n",
      "a new, automatically generated dataset, Compositional Celebrities (CC), of 8.6k 2-hop questions; it\n",
      "combines frequently stated facts in improbable ways (e.g., “Who won the Master’s Tournament the\n",
      "year Justin Bieber was born?”), allowing us to disentangle memorization and reasoning. Intuitively,\n",
      "the reasoning required to answer such questions seems trivial given access to the relevant facts.\n",
      "\n",
      "set of questions, and Bamboogle is designed to measure the extent to which a question answering\n",
      "system can answer varied compositional questions, albeit with less statistical power.\n",
      "Finally, we show that the structure of our prompting scheme combines easily with an internet search\n",
      "engine to further improve results on compositional questions.\n",
      "In summary, we systematically reveal that although LMs can sometimes compose two facts they\n",
      "observed separately during pretraining, they fail to do so in a large fraction of cases, even when they\n",
      "demonstrate knowledge of the constituent facts individually. We call this ratio the compositionality\n",
      "gap and show that it does not shrink with scale. We show that elicitive prompts, such as chain of\n",
      "thought and our self-ask prompting, narrow and sometimes close this gap and in general improve\n",
      "2the ability of LMs to solve complex compositional questions. In addition, self-ask can be easily\n",
      "combined with a search engine to further improve performance.\n",
      "\n",
      "questions, by reading random Wikipedia articles and writing a 2-hop question about them, leading\n",
      "to a varied dataset that challenges a system’s ability to decompose complex questions.\n",
      "We derive the 2-hop question for each article by querying two unrelated facts about the article’s\n",
      "topic. For example, while reading the article for V oyager 2, we learn that it was the ﬁrst probe\n",
      "to ever approach Uranus and that it was launched on a Titan IIIE rocket, leading to the question,\n",
      "“What rocket was the ﬁrst spacecraft that ever approached Uranus launched on?” We then run our\n",
      "questions through an internet search engine and add them to the ﬁnal dataset only if the query elicits\n",
      "an answer ‘featured snippet’ that is incorrect ; see Appendix Figure 6. We use a search engine to\n",
      "ﬁlter our dataset since we hypothesize that its inability to answer these questions indicates that the\n",
      "questions are not on the web. Appendix Table 6 presents more examples of Bamboogle questions.\n",
      "6GPT-3\n",
      "\n",
      "When did the president who set the precedent of a two term limit leave ofﬁce?\n",
      "Can people who have celiac eat camel meat?\n",
      "Who is the largest aircraft carrier in the world is named after?\n",
      "Who built the fastest air-breathing manned aircraft?\n",
      "The machine used to extract honey from honeycombs uses which physical force?\n",
      "In what year was the government department where the internet originated at founded?\n",
      "Who founded the city where the founder of geometry lived?\n",
      "Figure 6 shows an example of the popular internet search engine returning an incorrect “featured\n",
      "snippet” to a query containing a 2-hop question. To construct Bamboogle we manually wrote 2-\n",
      "hop questions and then checked whether they lead the search engine to return featured snippets that\n",
      "contain the wrong answer. This provides evidence that the given question does not appear on the\n",
      "web, and so would likely not appear in the training set of any LM.\n",
      "Human: What is the purpose of this document?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The purpose of this document is to present a study on the reasoning abilities of language models (LMs) in question answering tasks, specifically in answering compositional questions that require multi-hop reasoning. The study presents a new dataset of 2-hop questions, called Compositional Celebrities (CC), to evaluate the performance of LMs in answering such questions. The document also introduces a prompting scheme called Bamboogle, which is designed to measure the ability of a question answering system to answer varied compositional questions. The study shows that LMs struggle with compositional questions and that the prompting scheme can improve their performance.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the purpose of this document?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "answer = chain.run(input_documents=docs, question=query, verbose=True)\n",
    "# Printing the answer returned by the QA chain model\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
